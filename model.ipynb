{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import fitz \n",
    "import re\n",
    "from datasets import Dataset\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_paths():\n",
    "    conn = mysql.connector.connect(\n",
    "        host=\"localhost\",\n",
    "        user=\"root\",\n",
    "        password=\"RootUser@123\",\n",
    "        database=\"pdf_database\"\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT file_path FROM pdf_files\")\n",
    "    pdf_paths = cursor.fetchall()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    \n",
    "    # Print the obtained PDF paths\n",
    "    print(\"PDF paths obtained from the database:\")\n",
    "    for path in pdf_paths:\n",
    "        print(path[0])  # printing the file path\n",
    "    return [path[0] for path in pdf_paths]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdfs(pdf_paths):\n",
    "    text = \"\"\n",
    "    print(f\"Reading {len(pdf_paths)} PDF(s)...\")\n",
    "    for pdf_path in pdf_paths:\n",
    "        print(f\"Reading PDF file: {pdf_path}\")\n",
    "        doc = fitz.open(pdf_path)\n",
    "        for page in doc:\n",
    "            text += page.get_text(\"text\")\n",
    "    print(\"Finished reading PDFs.\")\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(all_text):\n",
    "    print(\"Cleaning the extracted text...\")\n",
    "    text = re.sub(r\"\\b(page \\d+|footnote|Header)\\b\", \"\", all_text)\n",
    "    text = text.strip()\n",
    "    print(\"Text cleaning complete.\")\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(cleaned_text):\n",
    "    data = [{\"text\": cleaned_text}]\n",
    "    dataset = Dataset.from_dict({\"text\": [entry[\"text\"] for entry in data]})\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(dataset, tokenizer):\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_labels(examples):\n",
    "    examples[\"labels\"] = examples[\"input_ids\"]\n",
    "    return examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer():\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(tokenized_dataset, model, tokenizer):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./gpt2_finetuned\",          # Output directory\n",
    "        num_train_epochs=1,                     # Number of training epochs\n",
    "        per_device_train_batch_size=2,          # Batch size per device during training\n",
    "        per_device_eval_batch_size=2,           # Batch size for evaluation\n",
    "        warmup_steps=500,                       # Number of warmup steps for learning rate scheduler\n",
    "        weight_decay=0.01,                      # Strength of weight decay\n",
    "        logging_dir=\"./logs\",                   # Directory for storing logs\n",
    "        logging_steps=10,\n",
    "        evaluation_strategy=\"epoch\",            # Evaluation strategy to adopt during training\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,                            # The model to be trained\n",
    "        args=training_args,                     # Training arguments\n",
    "        train_dataset=tokenized_dataset,        # Training dataset\n",
    "        eval_dataset=tokenized_dataset,         # Evaluation dataset\n",
    "    )\n",
    "\n",
    "    trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the model has been fine-tuned, save it along with the tokenizer\n",
    "model.save_pretrained(\"./gpt2_finetuned\")\n",
    "tokenizer.save_pretrained(\"./gpt2_finetuned\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
